<script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <a href="https://eccv.ecva.net/">ECCV2024</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/PdisconetV2-main-480.webp 480w,/assets/img/publication_preview/PdisconetV2-main-800.webp 800w,/assets/img/publication_preview/PdisconetV2-main-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/PdisconetV2-main.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="PdisconetV2-main.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="aniraj2024pdiscoformerrelaxingdiscoveryconstraints" class="col-sm-8"> <div class="title">PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers</div> <div class="author"> <em>Ananthu Aniraj</em>,&nbsp;<a href="https://cassiofragadantas.github.io/">Cassio F. Dantas</a>,&nbsp;<a href="https://scholar.google.com/citations?user=C8zfH3kAAAAJ">Dino Ienco</a>,&nbsp;and&nbsp;<a href="https://scholar.google.com/citations?user=IUqydU0AAAAJ">Diego Marcos</a> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2407.04538v3" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2407.04538" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://arxiv.org/pdf/2407.04538" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/ananthu-aniraj/pdiscoformer" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Computer vision methods that explicitly detect object parts and reason on them are a step towards inherently interpretable models. Existing approaches that perform part discovery driven by a fine-grained classification task make very restrictive assumptions on the geometric properties of the discovered parts; they should be small and compact. Although this prior is useful in some cases, in this paper we show that pre-trained transformer-based vision models, such as self-supervised DINOv2 ViT, enable the relaxation of these constraints. In particular, we find that a total variation (TV) prior, which allows for multiple connected components of any size, substantially outperforms previous work. We test our approach on three fine-grained classification benchmarks: CUB, PartImageNet and Oxford Flowers, and compare our results to previously published methods as well as a re-implementation of the state-of-the-art method PDiscoNet with a transformer-based backbone. We consistently obtain substantial improvements across the board, both on part discovery metrics and the downstream classification task, showing that the strong inductive biases in self-supervised ViT models require to rethink the geometric priors that can be used for unsupervised part discovery.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">aniraj2024pdiscoformerrelaxingdiscoveryconstraints</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Aniraj, Ananthu and Dantas, Cassio F. and Ienco, Dino and Marcos, Diego}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2407.04538}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">US-Patent</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/calving_monitoring-480.webp 480w,/assets/img/publication_preview/calving_monitoring-800.webp 800w,/assets/img/publication_preview/calving_monitoring-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/calving_monitoring.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="calving_monitoring.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="meeuwesen_system_2024" class="col-sm-8"> <div class="title">System for monitoring a calving mammal</div> <div class="author"> Adrianus Cornelis Maria MEEUWESEN,&nbsp;<a href="https://www.linkedin.com/in/yan-li-49a60253">Yan Li</a>,&nbsp;and&nbsp;<em>Ananthu ANIRAJ</em> </div> <div class="periodical"> Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://patents.google.com/patent/US20240090990A1/en" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://patentimages.storage.googleapis.com/fe/b6/7c/069283addbeba7/US20240090990A1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>A calving monitoring system for monitoring an animal at the end of the expected gestation period comprises a camera device for repeatedly taking images of the animal, a control unit for generating calving information from the images taken, and an alert device for sending an alert message according to the calving information generated. The control unit is configured so as, in each image taken, to recognise an animal image, to segment said animal image into multiple animal parts including a torso, to determine a parameter value relating to first pixels of said torso in the taken images as a time-dependent parametric function, wherein the parameter value comprises a width value of the torso, and to detect contractions when said parameter value meets a predetermined contraction criterion. The criterion is that the parameter value exhibits at least two peaks in said predetermined time duration which have at least a predetermined minimum width and/or a predetermined minimum height. The control unit generates calving information which comprises an indicator of the contractions detected.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@patent</span><span class="p">{</span><span class="nl">meeuwesen_system_2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{System for monitoring a calving mammal}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://patents.google.com/patent/US20240090990A1/en}</span><span class="p">,</span>
  <span class="na">nationality</span> <span class="p">=</span> <span class="s">{US}</span><span class="p">,</span>
  <span class="na">assignee</span> <span class="p">=</span> <span class="s">{Lely Patent NV}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{US20240090990A1}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-07-29}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{MEEUWESEN, Adrianus Cornelis Maria and Li, Yan and ANIRAJ, Ananthu}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{animal, calving, contractions, control unit, torso}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <a href="https://www.computer.org/csdl/proceedings/iccvw/2023/1TangBBH5cI">ICCVW2023</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/LM_EM_alternate-480.webp 480w,/assets/img/publication_preview/LM_EM_alternate-800.webp 800w,/assets/img/publication_preview/LM_EM_alternate-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/LM_EM_alternate.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="LM_EM_alternate.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10351024" class="col-sm-8"> <div class="title">Masking Strategies for Background Bias Removal in Computer Vision Models</div> <div class="author"> <em>Ananthu Aniraj</em>,&nbsp;<a href="https://cassiofragadantas.github.io/">Cassio F. Dantas</a>,&nbsp;<a href="https://scholar.google.com/citations?user=C8zfH3kAAAAJ">Dino Ienco</a>,&nbsp;and&nbsp;<a href="https://scholar.google.com/citations?user=IUqydU0AAAAJ">Diego Marcos</a> </div> <div class="periodical"> <em>In 2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2308.12127" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Aniraj_Masking_Strategies_for_Background_Bias_Removal_in_Computer_Vision_Models_ICCVW_2023_paper.html" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://openaccess.thecvf.com/content/ICCV2023W/OODCV/papers/Aniraj_Masking_Strategies_for_Background_Bias_Removal_in_Computer_Vision_Models_ICCVW_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/ananthu-aniraj/masking_strategies_bias_removal" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10351024</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Aniraj, Ananthu and Dantas, Cassio F. and Ienco, Dino and Marcos, Diego}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Masking Strategies for Background Bias Removal in Computer Vision Models}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4399-4407}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{computer vision;computational modeling;transformers;data models;robustness;convolutional neural networks;task analysis}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICCVW60793.2023.00474}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.ieeecomputersociety.org/10.1109/ICCVW60793.2023.00474}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE Computer Society}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Los Alamitos, CA, USA}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">WO-Patent</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/animal_husbandry-480.webp 480w,/assets/img/publication_preview/animal_husbandry-800.webp 800w,/assets/img/publication_preview/animal_husbandry-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/animal_husbandry.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="animal_husbandry.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li_animal_2023" class="col-sm-8"> <div class="title">Animal husbandry system</div> <div class="author"> <a href="https://www.linkedin.com/in/yan-li-49a60253">Yan Li</a>,&nbsp;Paulus Jacobus Maria Van Adrichem,&nbsp;<em>Ananthu ANIRAJ</em>,&nbsp;Cagla SOZEN, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Joram Robin VAN DER SLUIS, Sjoerd Timo VAN VLIET' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://patents.google.com/patent/WO2023203459A1/en" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://patentimages.storage.googleapis.com/fe/c6/81/dbcdb2afc66e0f/WO2023203459A1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The present invention relates to an animal husbandry system with monitoring and analyzing means configured for repeatedly generating images and gathering animal data therefrom. A plurality of cameras is suitable for monitoring the area from above. Image processing means are suitable for matching animals in the field of view of multiple cameras by detecting animal silhouettes in images of different cameras; defining an animal bounding box around each animal silhouette, applying to each animal bounding box either a first projection onto an image stitching plane if the animal is standing, or a second projection if the animal is lying down, stitching the projected bounding boxes in the stitching plane and identifying identical animals in the stitched image. The reliability of the matching process is increased by taking into account whether the animal in question is standing or lying down. The quality of the data gathered in the system is improved.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@patent</span><span class="p">{</span><span class="nl">li_animal_2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Animal husbandry system}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://patents.google.com/patent/WO2023203459A1/en}</span><span class="p">,</span>
  <span class="na">nationality</span> <span class="p">=</span> <span class="s">{WO}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">assignee</span> <span class="p">=</span> <span class="s">{Lely Patent N.V.}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{WO2023203459A1}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-07-29}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Yan and Adrichem, Paulus Jacobus Maria Van and ANIRAJ, Ananthu and SOZEN, Cagla and SLUIS, Joram Robin VAN DER and VLIET, Sjoerd Timo VAN}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{animal, animals, image, processing means, standing}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div>