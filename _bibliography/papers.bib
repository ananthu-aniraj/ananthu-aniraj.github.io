---
---

@INPROCEEDINGS {10351024,
author = {A. Aniraj and C. F. Dantas and D. Ienco and D. Marcos},
booktitle = {2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
title = {Masking Strategies for Background Bias Removal in Computer Vision Models},
year = {2023},
volume = {},
issn = {},
pages = {4399-4407},
abstract = {Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.},
keywords = {computer vision;computational modeling;transformers;data models;robustness;convolutional neural networks;task analysis},
doi = {10.1109/ICCVW60793.2023.00474},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCVW60793.2023.00474},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct},
arxiv = {2308.12127},
code = {https://github.com/ananthu-aniraj/masking_strategies_bias_removal},
pdf = {https://openaccess.thecvf.com/content/ICCV2023W/OODCV/papers/Aniraj_Masking_Strategies_for_Background_Bias_Removal_in_Computer_Vision_Models_ICCVW_2023_paper.pdf},
selected = {true},
}

@misc{aniraj2024pdiscoformerrelaxingdiscoveryconstraints,
      title={PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers}, 
      author={Ananthu Aniraj and Cassio F. Dantas and Dino Ienco and Diego Marcos},
      year={2024},
      abstract={Computer vision methods that explicitly detect object parts and reason on them are a step towards inherently interpretable models. Existing approaches that perform part discovery driven by a fine-grained classification task make very restrictive assumptions on the geometric properties of the discovered parts; they should be small and compact. Although this prior is useful in some cases, in this paper we show that pre-trained transformer-based vision models, such as self-supervised DINOv2 ViT, enable the relaxation of these constraints. In particular, we find that a total variation (TV) prior, which allows for multiple connected components of any size, substantially outperforms previous work. We test our approach on three fine-grained classification benchmarks: CUB, PartImageNet and Oxford Flowers, and compare our results to previously published methods as well as a re-implementation of the state-of-the-art method PDiscoNet with a transformer-based backbone. We consistently obtain substantial improvements across the board, both on part discovery metrics and the downstream classification task, showing that the strong inductive biases in self-supervised ViT models require to rethink the geometric priors that can be used for unsupervised part discovery.},
      eprint={2407.04538},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      arxiv={2407.04538v3},
      code={https://github.com/ananthu-aniraj/pdiscoformer},
      pdf={https://arxiv.org/pdf/2407.04538},
      selected={true},
}