---
---

@inproceedings{10351024,
abbr={ICCVW2023},
author = {Ananthu Aniraj and Cassio F. Dantas and Dino Ienco and Diego Marcos},
booktitle = {2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
title = {Masking Strategies for Background Bias Removal in Computer Vision Models},
year = {2023},
volume = {},
issn = {},
pages = {4399-4407},
abstract = {Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.},
keywords = {computer vision;computational modeling;transformers;data models;robustness;convolutional neural networks;task analysis},
doi = {10.1109/ICCVW60793.2023.00474},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCVW60793.2023.00474},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct},
arxiv = {2308.12127},
html = {https://openaccess.thecvf.com/content/ICCV2023W/OODCV/html/Aniraj_Masking_Strategies_for_Background_Bias_Removal_in_Computer_Vision_Models_ICCVW_2023_paper.html},
code = {https://github.com/ananthu-aniraj/masking_strategies_bias_removal},
pdf = {https://openaccess.thecvf.com/content/ICCV2023W/OODCV/papers/Aniraj_Masking_Strategies_for_Background_Bias_Removal_in_Computer_Vision_Models_ICCVW_2023_paper.pdf},
selected = {true},
bibtex_show={true},
preview={LM_EM_alternate.png},
}

@inproceedings{aniraj2024pdiscoformer,
      abbr={ECCV2024},
      title={PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers}, 
      author={Ananthu Aniraj and Cassio F. Dantas and Dino Ienco and Diego Marcos},
      booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  	  month={oct},
  	  year={2024},
      abstract={Computer vision methods that explicitly detect object parts and reason on them are a step towards inherently interpretable models. Existing approaches that perform part discovery driven by a fine-grained classification task make very restrictive assumptions on the geometric properties of the discovered parts; they should be small and compact. Although this prior is useful in some cases, in this paper we show that pre-trained transformer-based vision models, such as self-supervised DINOv2 ViT, enable the relaxation of these constraints. In particular, we find that a total variation (TV) prior, which allows for multiple connected components of any size, substantially outperforms previous work. We test our approach on three fine-grained classification benchmarks: CUB, PartImageNet and Oxford Flowers, and compare our results to previously published methods as well as a re-implementation of the state-of-the-art method PDiscoNet with a transformer-based backbone. We consistently obtain substantial improvements across the board, both on part discovery metrics and the downstream classification task, showing that the strong inductive biases in self-supervised ViT models require to rethink the geometric priors that can be used for unsupervised part discovery.},
      arxiv={2407.04538v3},
      code={https://github.com/ananthu-aniraj/pdiscoformer},
      pdf={https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11397.pdf},
	  html={https://eccv.ecva.net/virtual/2024/poster/124},
      selected={true},
	  bibtex_show={true},
	  preview={PdisconetV2-main.png},
}

@patent{li_animal_2023,
	abbr={WO-Patent},
	title = {Animal husbandry system},
	url = {https://patents.google.com/patent/WO2023203459A1/en},
	html = {https://patents.google.com/patent/WO2023203459A1/en},
	abstract={The present invention relates to an animal husbandry system with monitoring and analyzing means configured for repeatedly generating images and gathering animal data therefrom. A plurality of cameras is suitable for monitoring the area from above. Image processing means are suitable for matching animals in the field of view of multiple cameras by detecting animal silhouettes in images of different cameras; defining an animal bounding box around each animal silhouette, applying to each animal bounding box either a first projection onto an image stitching plane if the animal is standing, or a second projection if the animal is lying down, stitching the projected bounding boxes in the stitching plane and identifying identical animals in the stitched image. The reliability of the matching process is increased by taking into account whether the animal in question is standing or lying down. The quality of the data gathered in the system is improved.},
	nationality = {WO},
	language = {en},
	assignee = {Lely Patent N.V.},
	number = {WO2023203459A1},
	urldate = {2024-07-29},
	author = {Li, Yan and Adrichem, Paulus Jacobus Maria Van and ANIRAJ, Ananthu and SOZEN, Cagla and SLUIS, Joram Robin VAN DER and VLIET, Sjoerd Timo VAN},
	month = {oct},
	year = {2023},
	keywords = {animal, animals, image, processing means, standing},
	pdf = {https://patentimages.storage.googleapis.com/fe/c6/81/dbcdb2afc66e0f/WO2023203459A1.pdf},
    selected={false},
    bibtex_show={true},
	preview={animal_husbandry.png},
}


@patent{meeuwesen_system_2024,
	abbr={US-Patent},
	title = {System for monitoring a calving mammal},
	url = {https://patents.google.com/patent/US20240090990A1/en},
	html = {https://patents.google.com/patent/US20240090990A1/en},
	abstract={A calving monitoring system for monitoring an animal at the end of the expected gestation period comprises a camera device for repeatedly taking images of the animal, a control unit for generating calving information from the images taken, and an alert device for sending an alert message according to the calving information generated. The control unit is configured so as, in each image taken, to recognise an animal image, to segment said animal image into multiple animal parts including a torso, to determine a parameter value relating to first pixels of said torso in the taken images as a time-dependent parametric function, wherein the parameter value comprises a width value of the torso, and to detect contractions when said parameter value meets a predetermined contraction criterion. The criterion is that the parameter value exhibits at least two peaks in said predetermined time duration which have at least a predetermined minimum width and/or a predetermined minimum height. The control unit generates calving information which comprises an indicator of the contractions detected.},
	nationality = {US},
	assignee = {Lely Patent NV},
	number = {US20240090990A1},
	urldate = {2024-07-29},
	author = {MEEUWESEN, Adrianus Cornelis Maria and Li, Yan and ANIRAJ, Ananthu},
	month = {mar},
	year = {2024},
	keywords = {animal, calving, contractions, control unit, torso},
	pdf = {https://patentimages.storage.googleapis.com/fe/b6/7c/069283addbeba7/US20240090990A1.pdf},
    selected={false},
	bibtex_show={true},
	preview={calving_monitoring.png},
}



